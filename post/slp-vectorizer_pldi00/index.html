<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Paper Reading | Exploiting Superword Level Parallelism with Multimedia Instruction Sets - Enna1's website</title><script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Paper Reading | Exploiting Superword Level Parallelism with Multimedia Instruction Sets"><meta property="og:description" content="本文是论文《Exploiting Superword Level Parallelism with Multimedia Instruction Sets》的阅读笔记。论文提出一种称作 SLP 的向量化技术，作者是 Samuel Larsen 和 Saman Amarasinghe ，发表在 PLDI'2000。"><meta property="og:type" content="article"><meta property="og:url" content="https://enna1.github.io/post/slp-vectorizer_pldi00/"><meta property="article:section" content="post"><meta property="article:published_time" content="2022-02-02T00:00:00+00:00"><meta property="article:modified_time" content="2022-02-02T00:00:00+00:00"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel="shortcut icon" href=/favicon.ico></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Enna1's website" rel=home><div class="logo__item logo__text"><div class=logo__title>Enna1's website</div><div class=logo__tagline>Nobody dies a virgin, life fucks us all!</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/><span class=menu__text>home</span></a></li><li class=menu__item><a class=menu__link href=/post/><span class=menu__text>blog</span></a></li><li class=menu__item><a class=menu__link href=/random/><span class=menu__text>random</span></a></li><li class=menu__item><a class=menu__link href=/podcast/><span class=menu__text>podcast</span></a></li><li class=menu__item><a class=menu__link href=/archives/><span class=menu__text>archives</span></a></li><li class=menu__item><a class=menu__link href=/about/><span class=menu__text>about</span></a></li></ul></nav></div></header><link rel=stylesheet href=/js/katex/katex.min.css><script src=/js/katex/katex.min.js></script><script src=/js/katex/contrib/auto-render.min.js></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}]})})</script><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>Paper Reading | Exploiting Superword Level Parallelism with Multimedia Instruction Sets</h1><div class="post__meta meta"><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2022-02-02T00:00:00Z>February 02, 2022</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/programming/ rel=category>Programming</a></span></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#about-slp-superword-level-parallelism>About SLP (Superword Level Parallelism)</a></li><li><a href=#compared-to-previous-approach>Compared To Previous Approach</a></li><li><a href=#slp-extraction-algorithm>SLP Extraction Algorithm</a><ul><li><a href=#identifying-adjacent-memory-references>Identifying Adjacent Memory References</a></li><li><a href=#extending-the-packset>Extending the PackSet</a></li><li><a href=#combination>Combination</a></li><li><a href=#scheduling>Scheduling</a></li><li><a href=#example>Example</a></li></ul></li><li><a href=#implementation>Implementation</a></li><li><a href=#references>References</a></li></ul></nav></div></div><div class="content post__content clearfix"><p>本文是论文<a href=https://groups.csail.mit.edu/cag/slp/SLP-PLDI-2000.pdf>《Exploiting Superword Level Parallelism with Multimedia Instruction Sets》</a>的阅读笔记。论文提出一种称作 SLP 的向量化技术，作者是 Samuel Larsen 和 Saman Amarasinghe ，发表在 PLDI'2000。</p><h2 id=about-slp-superword-level-parallelism>About SLP (Superword Level Parallelism)</h2><p>SLP 即 Superword Level Parallelism，是自动向量化技术的一种（另一种是 Loop vectorizer）。SLP vectorization 的目标是将多条 independent isomorphic 指令组合成一条向量化指令。</p><p>例如，图 1 中的四条语句对应位置的操作数都可以 pack 到一个向量寄存器 ( vector register ) 中（b, e, s, x 被 pack 到一个向量寄存器中，c, f, t, y 被 pack 到一个向量寄存器中，z[i+0], z[i+1], z[i+2], z[i+3] 被 pack 到一个向量寄存器中），然后就可以通过 SIMD 指令并行执行这四条语句。</p><p><img src=/blog/slp-vectorizer_pldi00/2022-02-01-20-56-18-image.png alt></p><p>因为通过 SIMD 指令并行执行得到的结果也是在向量寄存器中的，所以根据 a, d, r, w 的（被）使用方式，可能还需要将 a, d, r, w 从向量寄存器中 load 出来。该操作称为 unpack。</p><p>如果 并行执行的时间开销 + packing 操作数时间开销 + unpacking 操作数时间开销 <strong>小于</strong> 原本执行的时间开销，就说明 SLP vectorization 有性能收益。</p><p>总结一下 SLP：</p><ul><li><p>Generally applicable: SLP is not restricted on parallelism of loops</p></li><li><p>Find independent isomorphic instructions within basic block</p></li><li><p>Goal</p><ol><li><p>Gain more speed up via parallelism</p></li><li><p>Minimize the cost of packing and unpacking</p></li></ol></li><li><p>Prefer operating on adjacent memory, whose cost of packing is minimum</p></li></ul><h2 id=compared-to-previous-approach>Compared To Previous Approach</h2><p>在作者撰写论文时，向量编译器 (vector compilers) 通常以循环为目标来寻找 vector parallelism 的机会，因为循环天然提供了对多个数据执行相同指令的机会。向量编译器通过 loop transformations 将一段代码转换为可以被向量化的形式 (vectorizable form)。</p><p>例如，如下循环：</p><pre tabindex=0><code>for (i=0; i&lt;16; i++) {
  localdiff = ref[i] - curr[i];
  diff += abs(localdiff);
}
</code></pre><p>应用 scalar expansion 和 loop fission 后就被转换为了可以被向量化的形式：</p><pre tabindex=0><code>for (i=0; i&lt;16; i++) {
  T[i] = ref[i] - curr[i];
}
for (i=0; i&lt;16; i++) {
  diff += abs(T[i]);
}
</code></pre><p>注意：应用了 scalar expansion 和 loop fission 后的代码，只有第一个循环是可以通过 SIMD 指令一次执行多次减法操作的，第二个循环则不能。</p><p>SLP 同样能够对上循环进行向量化，并且是以一个完全不同的角度：</p><p>如下循环：</p><pre tabindex=0><code>for (i=0; i&lt;16; i++) {
  localdiff = ref[i] - curr[i];
  diff += abs(localdiff);
}
</code></pre><p>经过 loop unroll 和 rename 后得到：</p><pre tabindex=0><code>for (i=0; i&lt;16; i+=4) {
  localdiff0 = ref[i+0] - curr[i+0];
  diff += abs(localdiff0);

  localdiff1 = ref[i+1] - curr[i+1];
  diff += abs(localdiff1);

  localdiff2 = ref[i+2] - curr[i+2];
  diff += abs(localdiff2);

  localdiff3 = ref[i+3] - curr[i+3];
  diff += abs(localdiff3);
}
</code></pre><p>这样 SLP 就能够将计算 localdiff{0, 1, 2, 3} 的这四条 independent isomorphic 指令组合成一条向量化指令 (SIMD-)：</p><pre tabindex=0><code>for (i=0; i&lt;16; i+=4) {
  localdiff0 = ref[i+0] - curr[i+0];
  localdiff1 = ref[i+1] - curr[i+1];
  localdiff2 = ref[i+2] - curr[i+2];
  localdiff3 = ref[i+3] - curr[i+3];

  diff += abs(localdiff0);
  diff += abs(localdiff1);
  diff += abs(localdiff2);
  diff += abs(localdiff3);
}
</code></pre><hr><p><strong>但是</strong>对于如下的代码片段，向量编译器 (vector compilers) 要想向量化该循环，需要将 do while 循环转换为 for 循环，恢复归纳变量 (induction variable) ，将展开后的循环恢复为未展开的形式 (loop rerolling)。而 SLP 向量化该循环则非常容易，直接将计算 dst[{0, 1, 2, 3}] 的这四条 independent isomorphic 语句组合成一条使用向量化指令的语句即可。</p><pre tabindex=0><code>do {
  dst[0] = (src1[0] + src2[0]) &gt;&gt; 1;
  dst[1] = (src1[1] + src2[1]) &gt;&gt; 1;
  dst[2] = (src1[2] + src2[2]) &gt;&gt; 1;
  dst[3] = (src1[3] + src2[3]) &gt;&gt; 1;

  dst  += 4;
  src1 += 4;
  src2 += 4;
}
</code></pre><h2 id=slp-extraction-algorithm>SLP Extraction Algorithm</h2><p>作者提出了一种简单的算法，将具有 SLP 机会的基本块转换为使用 SIMD 指令的基本块。该算法寻找 independent（无数据依赖 ）、isomorphic（相同操作）的指令组合成一条向量化指令。</p><p>作者观察到 (observation)：</p><blockquote><p>Packed statements that containt adjacent memory references among corresponding operands are particularly well suited for SLP execution</p></blockquote><p>即 如果被 pack 的指令的操作数引用的是相邻的内存，那么则特别适合 SLP 执行。</p><p>所以 SLP Extraction Algorithm 的核心算法就是从识别 adjacent memory references 开始的。</p><p>在识别 adjacent memory references 开始之前实际上还有一些准备工作要做：</p><ol><li><p><strong>Loop unrolling.</strong> transform vector parallelism into basic blocks with superword level parallelism，见 <a href=#Compared-To-Previous-Approach>Compared To Previous Approach</a></p></li><li><p><strong>Alignment analysis.</strong> memory load, store, simd</p></li><li><p><strong>Pre-Optimization.</strong> constant propagation, dead code elimination, common subexpression elimination, loop invariant code motion and redundant load/store elimination. 避免向量化不必要的代码（死代码、冗余代码）</p></li></ol><p>SLP Extraction Algorithm 的核心算法如下：</p><p><img src=/blog/slp-vectorizer_pldi00/2022-02-05-17-23-01-image.png alt></p><p>主要分为以下 4 步：</p><ol><li><p>Identifying Adjacent Memory References</p></li><li><p>Extending the PackSet</p></li><li><p>Combination</p></li><li><p>Scheduling</p></li></ol><p>下面进行详细解释。</p><h3 id=identifying-adjacent-memory-references>Identifying Adjacent Memory References</h3><p>Identifying Adjacent Memory References 即 find_adj_refs，伪代码如下：</p><p><img src=/blog/slp-vectorizer_pldi00/2022-02-05-17-26-19-image.png alt></p><p>find_adj_refs 的输入是 BasicBlock，输出为集合 PackSet。</p><p>对于 BasicBlock 中的任意语句对 &lt;s, s&rsquo;>，如果语句 s 和 s&rsquo; 访问了相邻的内存（如，s 访问了 array[1]， s&rsquo; 访问了 array[2]），并且语句 s 和 s&rsquo; 能 pack 到一起（函数 stmts_can_pack 返回 true ），那么就将语句对 &lt;s, s&rsquo;> 加入集合 PackSet 中。</p><p>函数 stmts_can_pack 的伪代码如下：</p><p><img src=/blog/slp-vectorizer_pldi00/2022-02-05-17-46-52-image.png alt></p><p>即，如果两条语句 s 和 s&rsquo; 满足如下条件，那么语句 s 和 s&rsquo; 就能 pack 到一起：</p><ul><li><p>s 和 s&rsquo; 是相同操作 (isomorphic)</p></li><li><p>s 和 s&rsquo; 无数据依赖 (independent)</p></li><li><p>s 之前没有作为左操作数出现在 PackSet 中，s&rsquo; 之前没有作为右操作数出现在 PackSet 中</p></li><li><p>s 和 s&rsquo; 的满足对齐要求 (consistent)</p></li></ul><p>find_adj_refs 执行结束后，我们就得到了集合 PackSet，PackSet 中元素是 &lt;s, s&rsquo;> 这样的语句对。</p><h3 id=extending-the-packset>Extending the PackSet</h3><p>在 find_adj_refs 我们构建了 PackSet 集合，在这一步中我们沿着被 pack 的语句的 defs 和 uses 来扩充 PackSet 集合。extent_packlist 的输入集合 PackSet，输出为集合 PackSet。</p><p>extent_packlist 的伪代码如下：</p><p><img src=/blog/slp-vectorizer_pldi00/2022-02-05-18-22-56-image.png alt></p><p>对 Packet 中每一个元素 Pack，执行函数 follow_use_defs 和 follow_def_uses 扩充 PackSet 集合，不断扩充直至 PackSet 不能再加入新的 Pack。</p><p>先看 follow_use_defs：</p><p><img src=/blog/slp-vectorizer_pldi00/2022-02-05-18-23-59-image.png alt></p><p>对于一个 Pack，即语句对 &lt;s, s&rsquo;>：考察 s 和 s&rsquo; 的每一对源操作数 xj 和 xj&rsquo;，如果 s 和 s&rsquo; 所在 BasicBlock 中存在对 xj 和 xj&rsquo; 定值 (def) 的语句 t 和 t&rsquo;，语句 t 和 t&rsquo; 还能 pack 到一起（函数 stmts_can_pack 返回 true ），并且根据 cost model，将 &lt;t, t&rsquo;> 加入 PackSet 中有收益，那么就将 &lt;t, t&rsquo;> 加入集合 PackSet 中。</p><p>再看 follow_def_uses：</p><p><img src=/blog/slp-vectorizer_pldi00/2022-02-05-18-24-23-image.png alt></p><p>对于一个 Pack，即语句对 &lt;s, s&rsquo;>：考察 s 和 s&rsquo; 的目的操作数 x0 和 x0&rsquo;，如果 s 和 s&rsquo; 所在 BasicBlock 中存在使用 (use) x0 和 x0&rsquo; 的语句 t 和 t&rsquo;，语句 t 和 t&rsquo; 还能 pack 到一起（函数 stmts_can_pack 返回 true ），根据 cost model，找到将 &lt;t, t&rsquo;> 加入 PackSet 后获得收益最大的使用 (use) x0 和 x0&rsquo; 的语句 u 和 u&rsquo;（存在多个使用 x0 和 x0&rsquo; 的语句 t 和 t&rsquo;），将 &lt;u, u&rsquo;> 加入集合 PackSet 中。</p><p>extent_packlist 执行结束后，我们就扩充了集合 PackSet，PackSet 中元素是 &lt;s, s&rsquo;> 这样的语句对。</p><h3 id=combination>Combination</h3><p>在 find_adj_refs 我们构建了 PackSet 集合，在 extent_packlist 中我们扩充了 PackSet 集合。此时 PackSet 中元素是 &lt;s, s&rsquo;> 这样的语句对。</p><p>这一步我们对 PackSet 中的语句对进行合并，combine_packs 的输入集合 PackSet，输出为集合 PackSet。伪代码如下：</p><p><img src=/blog/slp-vectorizer_pldi00/2022-02-05-18-42-14-image.png alt></p><p>对于 PackSet 中的任意两个 Pack，p = &lt;s1, &mldr;, sn>，p&rsquo; = &lt;s1&rsquo;, &mldr;, sm&rsquo;>，如果 p 的最后一个语句和 p&rsquo; 的第一个语句是同一个语句，那么就将 p 和 p&rsquo; 合并。</p><p>这一步很容易理解。 combine_packs 执行结束后，PackSet 中元素是 &lt;s, &mldr;, sn> 这样的语句 n 元组，n >= 2。</p><h3 id=scheduling>Scheduling</h3><p>最后一步对基本块中的指令进行调度，生成包含 SIMD 指令的基本块。 对于 PackSet 中的一个 Pack（Pack 是语句 n 元组），Pack 可能依赖于之前定义，因此我们需要按照数据依赖图的拓扑顺序生成指令。如果存在循环依赖，我们 revert 导致循环的 Pack 不在对该 Pack 使用 SIMD 指令。</p><p><img src=/blog/slp-vectorizer_pldi00/2022-02-05-18-55-39-image.png alt></p><p>Scheduling 这一步输入是原本的 BasicBlock 和 PackSet，输出是包含 SIMD 指令的 BasicBlock。</p><h3 id=example>Example</h3><p>这里我们用论文中的例子来理解一下整个算法的流程：</p><ol><li><p>初始状态，BasicBlock 中包含的指令序列如 (a) 所示。</p></li><li><p>执行 find_adj_refs，我们发现语句(1) 和 语句(4) 访问的分别是 a[i+0] 和 a[i+1]，并且满足 stmts_can_pack，所以将 &lt;(1), (4)> 加入到 PackSet 中。语句 (4) 和 语句(7) 访问的分别是 a[i+1] 和 a[i+2]，语句 (4) 和 语句(7) 是 independent 和 isomorphic 的，并且语句(4) 没有作为 Pack 的左操作数出现在 Pack 中（&lt;(1), (4)> 中语句(4) 是作为 Pack 的右操作数），语句(7) 也没有作为 Pack 的右操作数出现在 Pack 中，且语句 (4) 和 语句(7)满足对齐要求，所以再将 &lt;(4), (7)> 加入到 PackSet 中。
find_adj_refs 执行结束，此时 PackSet 内容为 {&lt;(1), (4)>, &lt;(4), (7)>}</p></li><li><p>执行 extent_packlist：</p><ol><li><p>follow_use_defs，在 BasicBlock 中没有对 a[i+0], a[i+1], a[i+2] 进行 def 的语句，所以第一次 follow_use_defs 没有改变 PackSet。</p></li><li><p>follow_def_uses，这一次将 (3) 和 (6)、(6) 和 (9) 加入到 PackSet 中，分别是根据 (1) 和 (4)、(4) 和 (7) follow_def_uses 得到的。</p></li><li><p>再一次执行 follow_use_defs，这一次将对 (3) 和 (6) 中定值 c 和 f 的语句(2) 和 (5) 加入到 PackSet 中，将对 (6) 和 (9) 中定值 f 和 j 的语句 (5) 和 (8) 加入到 PackSet 中。</p></li><li><p>再一次执行 follow_use_defs，发现没有新的 Pack 能加入到 PackSet 中了，extent_packlist 执行结束。</p></li></ol></li><li><p>执行 combine_packs：</p><ol><li><p>&lt;(1), (4)> 和 &lt;(4), (7)> 合并为 &lt;(1), (4), (7)></p></li><li><p>&lt;(3), (6)> 和 &lt;(6), (9)> 合并为 &lt;(3), (6), (9)></p></li><li><p>&lt;(2), (5)> 和 &lt;(5), (8)> 合并为 &lt;(2), (5), (8)></p></li></ol></li><li><p>执行 scheduling，注意 (3) 依赖 (1) 和 (2)，(6) 依赖 (4) 和 (5)，(9) 依赖 (7) 和 (8)。</p></li></ol><p><img src=/blog/slp-vectorizer_pldi00/2022-02-05-18-59-14-image.png alt></p><h2 id=implementation>Implementation</h2><p>LLVM <a href=https://llvm.org/docs/Vectorizers.html#the-slp-vectorizer>实现</a> 了 SLP vectorization 算法，是基于 &ldquo;Loop-Aware SLP in GCC&rdquo; by Ira Rosen, Dorit Nuzman, Ayal Zaks. 这篇论文。</p><p>后面会先阅读 Loop-Aware SLP in GCC 这篇论文，然后再学习 LLVM 的实现，写一下源码阅读笔记。</p><h2 id=references>References</h2><ol><li><p><a href=http://groups.csail.mit.edu/cag/slp/>http://groups.csail.mit.edu/cag/slp/</a></p></li><li><p><a href=https://www.cs.cornell.edu/courses/cs6120/2020fa/blog/slp/>https://www.cs.cornell.edu/courses/cs6120/2020fa/blog/slp/</a></p></li></ol></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/paper-reading/ rel=tag>Paper Reading</a></li><li class=tags__item><a class="tags__link btn" href=/tags/vectorizer/ rel=tag>Vectorizer</a></li></ul></div></footer></article></main><section class=comments><div id=disqus_thread></div><script type=application/javascript>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//enna1-github-io.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></section></div><aside class=sidebar><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><label><input class=widget-search__field type=search placeholder=SEARCH… name=q aria-label=SEARCH…></label>
<input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=https://enna1.github.io/></form></div><div class="widget-recent widget"><h4 class=widget__title>Recent Posts</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/post/learn-comdat-with-inline-functions/>Learn COMDAT with inline functions</a></li><li class=widget__item><a class=widget__link href=/post/learn-relocation-by-debugging-a-hwasan-linker-error/>Learn Relocation by Debugging a HWASAN Linker Error</a></li><li class=widget__item><a class=widget__link href=/random/2023-5-15-a-whirlwind-tour-of-the-llvm-optimizer/>A whirlwind tour of the LLVM optimizer</a></li><li class=widget__item><a class=widget__link href=/post/hwasan-internals/>HWASAN Internals</a></li><li class=widget__item><a class=widget__link href=/post/inside-asan-allocator/>Inside AddressSanitizer Allocator</a></li><li class=widget__item><a class=widget__link href=/random/2022-12-31-summary/>2022 年度总结</a></li><li class=widget__item><a class=widget__link href=/post/strict_aliasing-tbaa-and-type_sanitizer/>Strict Aliasing, TBAA and TypeSanitizer</a></li><li class=widget__item><a class=widget__link href=/post/dissecting-thread-sanitizer/>Dissecting ThreadSanitizer Algorithm</a></li><li class=widget__item><a class=widget__link href=/post/how-sanitizer-get-stacktrace/>How Sanitizer Get Stack Trace</a></li><li class=widget__item><a class=widget__link href=/post/how-sanitizer-interceptor-works/>How Sanitizer Interceptor Works</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>Categories</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/categories/programming/>Programming</a></li><li class=widget__item><a class=widget__link href=/categories/random/>Random</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>Tags</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=/tags/c++/ title=C++>C++</a>
<a class="widget-taglist__link widget__link btn" href=/tags/data-race/ title="Data Race">Data Race</a>
<a class="widget-taglist__link widget__link btn" href=/tags/llvm/ title=LLVM>LLVM</a>
<a class="widget-taglist__link widget__link btn" href=/tags/paper-reading/ title="Paper Reading">Paper Reading</a>
<a class="widget-taglist__link widget__link btn" href=/tags/sanitizer/ title=Sanitizer>Sanitizer</a>
<a class="widget-taglist__link widget__link btn" href=/tags/vectorizer/ title=Vectorizer>Vectorizer</a></div></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">Social</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=GitHub rel="noopener noreferrer" href=https://github.com/Enna1 target=_blank><svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0C85.9.0.0 85.8.0 191.7c0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2.0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8.0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7.0.0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4.0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5.0 25.6-.2 46.3-.2 52.6.0 5.1 3.5 11.1 13.2 9.2C329 348.2 384 276.4 384 191.7 384 85.8 298 0 192 0z"/></svg><span>GitHub</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Email href=mailto:xumingjie.enna1@bytedance.com><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg><span>xumingjie.enna1@bytedance.com</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title="Blog RSS" rel="noopener noreferrer" href=/post/index.xml target=_blank><svg class="widget-social__link-icon icon icon-rss" width="24" height="24" viewBox="0 0 448 512"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328.0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765.0 183.105.0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686.0 38.981.0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"/></svg><span>Blog RSS</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title="Podcast RSS" rel="noopener noreferrer" href=/podcast/index.xml target=_blank><svg class="widget-social__link-icon icon icon-rss" width="24" height="24" viewBox="0 0 448 512"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328.0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765.0 183.105.0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686.0 38.981.0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"/></svg><span>Podcast RSS</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2023 Enna1.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script></body></html>